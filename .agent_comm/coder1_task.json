{
  "agent_id": "coder1",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.HC_2508.10603v1_Why_Report_Failed_Interactions_With_Robots_Towar",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.HC_2508.10603v1_Why-Report-Failed-Interactions-With-Robots-Towar with content analysis. Detected project type: agent (confidence score: 6 matches).",
    "key_algorithms": [
      "Large-Language",
      "Hybrid",
      "Human-Centric",
      "Each",
      "Bcs",
      "Easy",
      "Foundation"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.HC_2508.10603v1_Why-Report-Failed-Interactions-With-Robots-Towar.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nWhy Report Failed Interactions With Robots?!\nTowards Vignette-based Interaction Quality\nAgnes Axelsson\nTU Delft\nDelft, Netherlands\na.axelsson@tudelft.nlMerle Reimann\nVU Amsterdam\nAmsterdam, Netherlands\nm.m.reimann@vu.nlRonald Cumbal\nUppsala University\nUppsala, Sweden\nronald.cumbal@it.uu.seHannah Pelikan\nLink\u00a8oping University\nLink \u00a8oping, Sweden\nhannah.pelikan@liu.seDivesh Lala\nOsaka University\nOsaka, Japan\nlala.divesh.kanu.es@\nosaka-u.ac.jp\nAbstract \u2014Although the quality of human-robot interactions\nhas improved with the advent of LLMs, there are still various\nfactors that cause systems to be sub-optimal when compared to\nhuman-human interactions. The nature and criticality of failures\nare often dependent on the context of the interaction and so\ncannot be generalized across the wide range of scenarios and\nexperiments which have been implemented in HRI research. In\nthis work we propose the use of a technique overlooked in the field\nof HRI, ethnographic vignettes, to clearly highlight these failures,\nparticularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create\nour own based on our personal experiences with failures in HRI\nsystems. We emphasize the strength of vignettes as the ability\nto communicate failures from a multi-disciplinary perspective,\npromote transparency about the capabilities of robots, and\ndocument unexpected behaviours which would otherwise be\nomitted from research reports. We encourage the use of vignettes\nto augment existing interaction evaluation methods.\nIndex Terms \u2014ethnographic vignettes, spoken interaction, dia-\nlogue, quality, HRI, HAI, reporting\nI. I NTRODUCTION\nHigh-quality dialogue with robots is a goal for many human-\nrobot interaction (HRI) researchers [38]. Despite technological\nadvancements, dialogues in HRI sometimes fail. In this paper,\nwe propose vignette-writing as a method for reporting obser-\nvations from failed interactions.\nThe abilities of large language models (LLMs) to simulate\nhuman language have sparked an increased interest and opti-\nmism towards generating meaningful dialogues, despite their\nwell-known shortcomings [6, 9, 24]. However, there is still\nmuch ground to cover towards flawless spoken interactions\nwith robots [45]. One of the challenges that needs to be\naddressed in order to move towards this goal lies in defining,\ndescribing and evaluating concrete interactions. In this paper,\nwe propose that describing moments of failure in dialogues\nthrough ethnographic methods is one path to understanding,\nevaluating and defining human-robot interactions. We draw\non the long history of human-machine dialogue in human-\ncomputer interaction (HCI) [44, 52, 71] and on work on\nspeech-enabled robots [45, 53, 63].\nAnalysing human-agent1spoken interactions is challenging.\nUnconstrained interactions will always lead to unexpected\n1For the purposes of this paper, an agent is a virtual or situated agent, while\narobot is strictly a situated embodied agent.edge cases which can affect the experience of the user in ways\nthat were not anticipated by the system designers. The users\u2019\nexpectations of the robot\u2019s capabilities, informed by embodi-\nment and multimodal capabilities, can also be unpredictable.\nThis leads to varying expectations of interaction quality [41].\nMore importantly, evaluating interaction typically involves\nqualitative [7, 61] or quantitative [21, 39] measurements that\nmay only capture specific aspects of the interaction, potentially\noverlooking subtly important factors. Given this, we need to\nreexamine the analysis and reporting of spoken HAI.\nFailures , a \u201cdegraded state of ability which causes the\nbehaviour or service being performed by the system to deviate\nfrom the ideal, normal or correct functionality\u201d [12, 64],\nare a particularly interesting phenomenon in HAI evaluation.\nInteractive agents can employ remediation strategies [36] to\nrepair the consequences if they are aware of a failure that\nis not too big to excuse or explain [18]. Which strategies\nare appropriate depends on the nature of the interaction [14].\nFailures consistently lead to human users losing trust in robots\n[23, 54]. Users may lose trust in their own ability to contribute\nto the interaction, or lose trust in the interaction itself [55].\nIn this paper, we propose the use of ethnographic vignettes\n[8, 25] \u2013 short descriptions of situated, contextualised interac-\ntions \u2013 to add to classic interaction assessment metrics by re-\nporting moments of failure in interactions. Vignettes let us re-\nport unexpected events and deviations from planned scenarios\nin HAI. As there is no consensus on what constitutes a good\norpoor dialogue interaction in human-agent interaction [22],\nqualitative approaches such as these vignettes are valuable\nfor capturing subjective observations during interactions. Such\nobservations can then provide insights that other researchers\nin the community may also find worthy of further analysis.\nWe demonstrate the use of ethnographic vignettes by draw-\ning from our personal experiences in building and observing\nrobots in interaction. We illustrate how vignette-based report-\ning can further raise awareness of challenges in HRI, allowing\nresearchers to document overlooked or \u201cunworthy\u201d issues.\nOur vignettes reflect design ethnography , or stories told to\ninfluence and reflect the stage of designing a system [19] and\nsee vignette writing as a tool for documenting observations by\npractitioners, as in educational sciences [4, 20].arXiv:2508.10603v1  [cs.RO]  14 Aug 2025\n\n--- Page 2 ---\nII. R ELATED WORK\nA. Quality in Human Interactions\nOne perspective on evaluating interactions between hu-\nmans is measuring how interaction partners achieve their\ncommunicative goals. Communicative behaviours that lead\nto the partners not achieving their joint goals are then poor\ninteractions, while those that lead to the partners achieving\ntheir goals quickly and efficiently (i.e. with few repairs [36])\nare indicative of a good interaction [15].\nWith his maxims of quantity, quality, relation and man-\nner in the cooperative principle , Grice [32] highlights that\ncontributions should be brief, true, relevant to the interaction\nand understandable to the people in the current interaction.\nLikewise, Clark and Allwood et al. argue that communication\nis a cooperative activity that is negotiated between both parties\nof an interaction, although the exact mechanisms through\nwhich this happens, as well as the independence of the speaker\nfrom the listener, are not fully agreed upon [1, 15]. Evaluat-\ning an interaction as an outsider, hence, becomes inherently\nsubjective, as the true validity of an interaction can only be\nanalysed from the perspectives of the participants\u2019 personal\nand shared goals.\nB. Evaluating Human-Robot Interactions\nUnlike humans as described by Grice [32], interactive\nmachines do not genuinely follow a cooperative dynamic or\ninherently adapt to the needs of the user [47]. Users can,\nhowever, easily adapt to the machine [67] \u2013 or to what they\nbelieve the machine is doing [16].\nEvaluating interactions between humans and agents is heav-\nily influenced by the dialogue system\u2019s characteristics and the\ntask associated with the interaction [22]. Numerous scales and\nmeasures have been proposed for the evaluation of interactions\nwith artificial social agents, although there is no standard\nevaluation procedure. Rather, evaluation methods vary signif-\nicantly between studies, since each method is often task and\ncontext dependent [17, 65]. Fitrianie et al. [26] and Bagchi\net al. [2] have criticised the low reuse of questionnaires at the\nIV A and HRI conferences respectively.\nFollowing the principles from Section II-A, a user\u2019s expe-\nrience of HRI can also be evaluated by measuring satisfaction\n[10, 13, 40], frustration [68], or the user\u2019s overall perception\nof the agent to gain insights into the interaction [29, 48].\nTools like the Godspeed questionnaire [3] or the extensive\n90-item Artificial-Social-Agent questionnaire [27] are often\nemployed. Post-interaction semi-structured or open interviews\n[51] complement the more static questionnaires.\nMoving away from the users\u2019 subjective experiences, quality\ncan be measured by the objective number of turns taken [42],\nthe interaction duration [43] or, for task-based interactions, the\ntask success rate [42]. Objective metrics offer the advantage\nthat the robot can be made to evaluate the quality of its own\ninteraction in real-time [46].\nBoth subjective and objective data are valuable for evalu-\nating interaction quality. These measures allow researchers tocompare interactions within studies or across different related\nworks. However, they often only capture the constructs the\nresearchers intended to measure, potentially overlooking many\nelements of the interaction that may have impacted it but were\nnot expected.\nC. Observing Human-Robot Interactions\nWe have described ways of measuring subjective and ob-\njective factors after an interaction, or objective factors dur-\ning an interaction. There is less work on documenting the\nquality of an entire interaction seen as a whole. An early\nstudy by Sabanovic et al. focused on documenting HRI in\nreal-world settings. The authors pushed for the observation\nof social robots in natural environments, demonstrating that\nthis approach highlighted significant issues in social robot\ndesign and challenged initial design assumptions, ultimately\nenhancing the robot\u2019s interaction qualities [57].\nMethodologies inspired by ethnological2research are useful\nfor describing a full interaction, but can be misunderstood or\nincorrectly perceived as lacking scientific validity [34]. Mutlu\nand Forlizzi analysed ethnographic data collected from an\nautonomous delivery robot in a hospital, revealing substantial\ndifferences in how individuals integrated the robot into their\nwork [49]. Forlizzi et al. showed varied reactions among users\ninteracting with cleaning robots in a home environment. The\nstudy revealed that social attribution to the robots encouraged\nmore rapid adoption and reduced the stigma associated with\nbeing reliant on the technology [28]. Sabelli et al. observed\na conversational robot in an elderly care center over a period\nof 3.5 months, gaining a deeper understanding of how elderly\nindividuals interacted with the robot. Interactions were framed\nin terms of the needs and activities of daily life [58].\nMany of these studies are framed as exploratory or initial\nsteps in longer research projects. It is essential to emphasise\nthat observational approaches can be valuable in allresearch\nevaluating and analysing human-agent interactions. As demon-\nstrated by these studies, the authors discovered unexpected\nresults that significantly enhanced the overall understanding\nof the interactions. While extensive qualitative studies are\nimportant to understand the wider societal impact of emerging\nsystems, in this paper we want to introduce a method that\nenables developers to formulate observations during proto-\ntyping, testing and demonstration. Looking at HRI by using\nethnomethodological conversation analysis helps identifying\nand analyzing different types of failures visible in video\nrecordings [50, 56, 66, 69].\nIII. M ETHODOLOGY\nEven though there are calls for focusing on failures in\nhuman-robot interaction [5, 11], they are not as consistently\nreported as intended or designed behaviours in scientific liter-\nature. By engaging in vignette-writing, we want to put more\nfocus on reporting moments in HRI in which interactional\ntrouble emerges from the perspective of the user, or in which\n2As ethnology is the study of society and humans, an ethnological perspec-\ntive on HRI means observing the robot as it fits into society.\n\n--- Page 3 ---\nthe interactions did not evolve as intended by design. This\nstands in the conversation analytic tradition of deviant case\nanalysis [59, 62], where deviations from the typical interaction\npattern are scrutinized with particular attention. By looking in\ndetail at cases that could be considered outliers , we hope to\nlearn more about the persisting problems that dialogue systems\nface in HRI.\nWe present our observations in the form of ethnographic\nvignettes [8, 25] focusing on illustrations of cases where\ninteractions fell short of their intended quality. Systematically\nworking through negative examples allows us to reason about\ninteraction quality in an inductive way. We do not claim that\nwe have found all problems that may possibly occur, but\nrather wish to contribute to a critical discourse that may enable\nHRI researchers to make informed design decisions while also\nconsidering HRI-specific challenges and limitations.\nGeneralization and quantity is not the concern for the illus-\ntrating examples; the fact that these cases happened provide\nevidence that they do occur [60]. We thoroughly discussed\neach of the vignettes in our diverse research team, ensuring\nthat the vignettes are related to spoken HRI, and describe\nanecdotal, but real, actionable and generalisable observations.\nA. How to Write Vignettes\nWriting vignettes requires a different writing style and\nauthor\u2019s voice than writing other scientific texts. They describe\nwhat the author observed and experienced [35] and are always\nshaped by the author\u2019s perspective. By properly formulating\nvignettes, observations can be mapped to social worlds and\npractices that are more generally understood. We describe\nthree general steps, adapted from [25], to equip people who are\nnot trained in anthropology or ethnomethodology to produce\nsystematic descriptions of their observations.\nImagine that you are describing the event to a friend. How\nwould you tell them about the event? What information would\nyou give to introduce the topic and let your friend imagine\nthemselves there? Because vignettes are a personal expression\nof an event and the takeaways from that event, they will look\ndifferent from person to person \u2013 some of the vignettes in\nSection IV illustrate this variation within the bounds described\nin this section.\n\u201cScenes on a page\u201d can be created in different ways [25,\np. 45]. Some writers prefer to jot down some notes first\nand elaborate later, while others produce longer sections of\ntext immediately. Writing vignettes is an iterative process , it\nmeans revising your vignette over and over. It can be helpful\nto come back to some hours later to fill in missing elements or\nto remove less essential details for mirroring what happened.\nPlan some time to get into the mood of writing, giving yourself\nspace to recall the event in detail.\nVignette writing is characterized by more details, adjectives\nand adverbs, especially when starting to write. As Goffman\n[31, p. 131] put it in a talk on fieldwork, \u201cas loose as that\nadverbialized prose is, it\u2019s still a richer matrix to start from\nthan stuff that gets reduced into a few words of \u2018sensible\nsentences\u2019. [...] to be scientific in this area, you\u2019ve got to startby trusting yourself and writing as fully and as lushly as you\ncan. [...] put yourself into situations that you write about so\nthat later on you will see how to qualify what it is you\u2019ve said.\nYou say, \u2018If felt that,\u2019, \u2018my feeling was,\u2019 \u2018I had a feeling that\u2019\n\u2014 that kind of thing. This is part of the self-discipline.\u201d The\nwriting is supposed to be reflective, so first-person pronouns\nare encouraged. It is usually better to add too much detail in\nthe first iterations and then gradually remove the parts that\nare not needed. Our informal guide to writing ethnographic\nvignettes for HRI is located in Appendix A.\nIV. V IGNETTES\nWe provide six vignettes that describe failures and reflect\non the broader current challenges for human-robot dialogue\nthat the vignettes point to.\nA. Challenges during the system development\nWhile working on a system, developers often test their\nsystems themselves in an ad hoc manner. Formulating these\nobservations in vignettes can be a valuable way to report on\ndesign avenues that were not pursued further, or to outline\nproblems encountered during testing that will likely cause\ntrouble during the real-world deployment.\n1) Vignette 1: LLM-generated study misinformation:\nWe were seting up build the robotic system to explore what\npurposes it could serve in public transit. In early testing, we\nexplored how to generate answers to unexpected questions by\npassengers. We speculated that a LLM could be applied to\nquickly generate answers to such questions based on TTS input\nfrom the passengers\u2019 speech, reducing the time to generate\nresponses since it would take an unreasonable amount of time\nfor the Wizard of Oz who was teleoperating our robot to type\nout an answer to each unexpected question.\nThe LLM was prompted with the relevant background\ndetails of the role it was supposed to play in our experiment. In\ntesting, when asked why it was in the vehicle, the LLM-driven\nrobot consistently responded with made-up details about the\nexperimental setup and the researchers behind the experiment.\nWhen asked who the researchers were, the system provided\nemail addresses and names of individuals who did not exist,\nbut whose affiliations and identities would have been plausible\nto the uninformed participants. Similarly, the LLM-powered\nrobot kept going beyond its prompted information when it was\nasked what the experiment was about, and added statements\nabout what research questions were being explored, although\nthis had not been asked for and was not part of the information\nto which it had access.\na) Reflection: The readiness with which the system was\nready to say something that we could tell was untrue, that\nthe passengers would not be able to tell was untrue, and\nthat it itself had no way of verifying, surprised us. We had\nexpected the LLM to fill the dead air with small talk or\nharmless filler, but instead its behaviour could have easily\nbeen harmful to our passengers\u2019 experience. We ended up\nhaving the Wizard type out these responses manually, with the\ndownside of significant delays. This is a context where Wizard\n\n--- Page 4 ---\nof Oz makes sense as a way for the designer to play along\nwith the user\u2019s ideas and in-situ brainstorming about what the\nsystem should be able to do. We prompted the LLM to not\nmake up details about the experiment, but the LLM\u2019s role\nwas explicitly to address situations where the question was\nunexpected and not something we could have a pre-written\nanswer for. Addressing such problems as they appear would\nbe an unending qualification problem [30].\n2) Vignette 2: Being unable to recognize the user\u2019s name:\nFor a lab opening, I was preparing a demo of a patient\ninterview robot. The code was already finished and its parts of\nit had been tested on their own with generic answers. However,\nduring the final tests, I tried to answer with different answers\nand a number of international colleagues tested the system as\nwell. The robot started the interaction by introducing itself and\nthen asking for the name of the user. When I or my colleagues\nprovided our names, the robot was unable to understand them\ncorrectly and either misunderstood them as other words, or\nasked for a repetition. I tried to get the robot to understand\nmy name by pronouncing it in various ways, but the robot\u2019s\nspeech recognition failed repeatedly. While we first laughed\nand saw it as a challenge to get the robot to say our names, it\nsoon became frustrating since we noticed that it was just not\npossible to use our real names in the conversation.\na) Reflection: This vignette illustrates how even a task\nthat appears simple can fail consistently. The behaviour of\nsingle modules, in this case the ASR, can impact the whole\ninteraction. The robot being unable to understand the user\u2019s\nname does not necessarily seem like a very impactful problem\nat first, but it clearly shows that the overall system is not\ninclusive. Especially if the robot is collecting data from the\nuser and is not just chit-chatting, it is important that the data\nis correct and that the user is able to provide the robot with\nthe correct information.\n3) Takeaways: These vignettes point to two crucial chal-\nlenges with dialogue - saying relevant things and hearing each\nother (see Section II-A). The LLM hallucinations described in\nVignette 1 break Grice\u2019s maxim of quality , where one should\nnot say what is not supported by evidence [32] \u2013 although it\nis unclear if an LLM can be said to hold a belief [47].\nB. Challenges while conducting an experiment\nEven if significant effort is spent on resolving interaction\nproblems during the development of a robotic system, issues\ncan arise at a completely different scale when users actually\nget to interact with it \u2013 in research, typically as it is used to\nrun an experiment. We illustrate how vignettes can help to\ndescribe aspects that were generally challenging throughout\nthe experiment - providing a way to report on what to improve\nthat does not invalidate the study as such. We also show\nhow vignettes can describe situations that would typically be\ndismissed as outliers, providing a way to discuss the larger\nchallenges that they point to.\n1) Vignette 3: Asking too long and complex questions:\nWe were conducting an experiment with elderly participants\ninteracting with an attentive listening robot in a rest home. Theparticipants talked with the robot about interesting experiences\nthey had. One participant spoke about their overseas holiday\nfor about 30 seconds. Then the robot responded by first\nacknowledging the information, explaining that they also liked\nthe destination, and then asked a question about what they\nthought about the cultural landmarks there. The response lasted\naround 10 seconds. We expected the participant to immediately\nanswer the question and continue speaking on that line of\nconversation. However the participant responded with \u201cHuh?\u201d\nand appeared confused. It appeared as if they could not catch\nwhat the robot was talking about. They proceeded with their\ntalk on another subject, ignoring the question from the robot.\na) Reflection: This vignette highlights that long turns,\neven ones meaningfully addressing the conversation, can be\ndifficult to follow. This may be especially problematic for\ngroups who may suffer more from hearing difficulties such as\nthe elderly. While it is possible to modify an LLM prompt to\nmitigate this issue, this does not guarantee that the complexity\nof a question will be reduced. Instead of long and complex\nutterances, users require an empathetic response from the\nsystem, which could be very simple, such as asking for\nexpansions (e.g. \u201cYou drove a car?\u201d) [15].\n2) Vignette 4: Inability to give contextually relevant follow-\nup information: As part of an experiment that I conducted, a\nrobot that we equipped with dialog capabilities was placed in\nthe wine aisle of a supermarket to give customers advice on\ntheir wine selections. A poster informed customers about the\nrobot\u2019s purpose of giving wine advice. The wine recommenda-\ntions were based on information like the price, ingredients and\nthe type of wine. After customers had successfully interacted\nwith the robot and received a wine recommendation, they\nwere presented with the general information of the wine and a\npicture. However, the robot did not display any information on\nwhere to find the wine on the shelves. When a customer asked\nthe robot where they could find the recommended wine, it was\nunable to provide that information, apart from mentioning the\ngeneral category of wine, which still left the customer with two\nshelves to search through by themselves. The customer voiced\ntheir frustration and did not start looking for the recommended\nwine. Instead of making use of the provided recommendation,\nthey decided to choose a wine themselves, completely ignoring\nthe previous interaction with the robot.\na) Reflection: This vignette highlights that not only the\nrobot\u2019s appearance but also the specific context that it is placed\nin may shape users\u2019 expectations. The customers expect a\nrobot capable of doing one thing (giving a recommendation)\nto also do the logical next thing (telling where to find the\nrecommended item). If a human is capable of recommending\na wine in a shop, then it is trivial for the human to also\ntell the customer where that wine is located in the shelf. As\nwe saw in this vignette, this assumption does not hold for\nrobots. System designers must be aware of what expectations\nusers will have from a system based on its environment,\nplacement and embodiment. LLMs may, in this case, give a\nmore naturally expressed dialogue, but would not help find\ninformation about where in the aisle a specific wine is placed.\n\n--- Page 5 ---\n3) Takeaways: Turn design, embodiment and multimodality\nremain challenges for human-robot dialogue as shown by\nthe vignettes. As described in Section II-A, humans come\nto human-robot interactions under the assumption that those\ninteractions will abide by basic communicative principles,\nwhether those be Gricean [32] or grounding-based [1, 15].\nIf the robot is not capable of aligning with the user\u2019s expec-\ntations, it falls on the user to adapt to the robot\u2019s style. In\nVignette 3, this would mean aligning with long utterances,\ncausing poor interaction quality. In Vignette 4, there is no\npossible alignment between a customer who wants the robot\nto perform a task that it is incapable of performing.\nC. Challenges while demonstrating a system to the public\nWhen systems are deployed, many interesting interactions\ncan be observed. In addition to more in-depth field studies,\nusing vignettes to report on observations during demos can\nbe a way to articulate persisting problems, which may point\nto weaknesses in the demonstrated system or serve as a\nmotivation (or informal bug reports) for a new iteration of\nsystem development.\n1) Vignette 5: Uttering situationally inappropriate re-\nsponses\nI was invited to a panel discussion on artificial intelligence\nby the university, and one of the co-panelists brought a robot\nusually deployed in a local technology museum to demonstrate\nhow LLMs can be used in robots. While preparing for the\npanel, I initiated a conversation with the robot, greeting it and\nasking what it could do. The setting was highly dynamic and\nnoisy, so I was aware that the robot might not be able to\nhandle the situation. While I expected that the robot might\nnot respond, or answer that it had not heard what I said\nproperly, I was instead surprised to find that the robot would\ngenerate a response in the noisy setting. When it responded,\nit became evident that the robot had not heard me correctly,\nas it responded \u201cI love you, too\u201d. Even though I laughed off\nthis unfitting statement in the moment, the scene came back\nto me when I later reflected on the event. The robot had\ndone something that could be categorized as harassment if\nits programmer (who was standing right beside me) had said\nit in this setting. While mishearing and producing unfitting\nresponses is common even in scripted robots, the fact that the\nresponse was generated by the LLM evoked new, unfamiliar\nfeelings. Was the programmer to blame in this case at all?\nWhat happened in between my utterance and the robot\u2019s LLM\nthat somehow considered this an adequate response?\na) Reflection: This vignette illustrates that LLMs may\npresent situationally inappropriate content, and it may be\nopaque to the user why this happened. While a scripted\nmuseum-robot might not say \u201cI love you\u201d, an LLM can easily\nproduce responses that violate the norms of the context where\nthey are deployed. In the case of LLMs, it becomes utterly\nnon-transparent who would be to blame \u2013 is it the developer\nof the robot, the LLM, or is the robot reproducing stereotypes\nthat came from its training data? One could set boundaries\nto prevent statements that might come off as inappropriate,but we hope to demonstrate with this example that a robot,\nwhich can more readily and proactively respond also has a\nmuch higher risk of generating utterances that are misfitting\nor experienced as potentially harassing, and that cannot be\neasily explained or recognized as an error in the automatic\nspeech recognition.\n2) Vignette 6: Acting contextually inappropriate: During a\nconference I attended, a humanoid robot with a display was\nplaced in the foyer to provide information about the location,\nthe conference and events. During a break, I decided to interact\nwith the robot to get some information about the conference\nevents that day.\nAfter approaching the robot and being greeted by it, I asked\nfor information about the conference. As a response, the robot\ndisplayed a long list of new options fitting the topic, while also\nproviding a verbal answer. I decided to already have a look\nat the options while listening to the response. However, what\nI did not notice was that the robot was using gestures while\ntalking. Since I was standing close to the robot and touching its\ntablet to scroll through the options, the robot hit me with one\nof its gestures. While the movement was not strong enough\nto hurt, it came as a surprise, since I had not been paying\nattention to the small random movements the robot had already\nperformed earlier. After this, the robot just continued without\nacknowledging what just happened.\na) Reflection: This vignette shows that spoken HRI is\nmore than just speech \u2013 it is also the combination of the speech\nwith other modalities. The robot used speech to communicate\nwith the user, but expected button presses on the tablet for\nselections. Even if those other modalities are used completely\ndisconnected from the speech, it does not mean that they do\nnot affect it. In the presented case, the complete disregard of\nhitting the user made the whole interaction feel surreal, since\nthe robot had clearly broken a social norm, by first hitting me\nand then not acknowledging its inappropriate behaviour.\n3) Takeaways: Vignette 5 and Vignette 6 illustrate the kinds\nof issues that arise when systems are demonstrated to the\npublic. The kinds of systems that are demonstrated will by\nnecessity be new and less developed than the more established\nsystems that the prospective users are used to. This means\nthat issues that had not been predicted by the designers and\ncreators will arise \u2013 like the ones seen in our vignettes. Such\nobservations could stimulate future research directions, since\nthey evoke more general questions about how interactions\nshould be like. However, currently these issues are likely to\ngo unreported, as it is hard to control the demo environment,\nrecord what happens (if the designers see the events to begin\nwith), and manage participant consent. Formulating vignettes\ncan be a way to capture the gist of what happened in a format\nthat can stimulate discussion and reflection.\nV. D ISCUSSION\nEthnographic vignettes can be both a way to report design\nproblems and unforeseen issues when testing a system. They\ncan serve as a way to report experiences that support design\ndecisions for how to address common reoccurring problems.\n\n--- Page 6 ---\nA multidisciplinary perspective is especially useful to figure\nout why a system that was built to fulfill a purpose does not\ndo so, particularly due to complex failures of missing common\nground [1, 15] between the designer, user and system.\nSemi-structured interviews help us access unstructured\nthoughts from participants without previously knowing what\nthose thoughts are. In the same way, ethnographic vignettes\ncan describe situations that appear during interactions with our\nexperiment for which we had not prepared. Vignettes written\nfrom a development perspective enable us to share interesting\nobservations which would otherwise not be part of the planned\nevaluation. They provide information in a descriptive way,\naccessible for researchers from other disciplines.\nIt takes effort, time and publication space to present vi-\ngnettes of interactions with a system. This needs to be weighed\nagainst how beneficial the vignettes are, but we argue that\nmany vignettes like the ones we presented in Section IV\nare less obvious than system designers believe that they\nare. Ethnographic vignettes should not replace quantitative\nanalysis, but augment it with more nuanced observations. In a\nqualitative evaluation of a system, vignettes may provide more\ninsights than aggregated user quotes from a survey.\nPublishing vignettes in a paper with a strict page/word\nlimit is challenging. They can end up filling up as much\nspace as a full paper, but would generally not be considered\nto contribute equally much. We suggest placing them in\nsupplemental materials to add context to the objectives of\nthe study. Some publication venues have started introducing\nadditional tracks where vignettes of system behaviour could\nbe a great contribution on their own. Tracks like the video\ntrack at HRI or the case study track at CHI, can be suitable\nalternatives for reporting on practical experiences with HRI\nand HCI respectively.\nVI. D ESIGN SUGGESTIONS\nSection V leads us to two main study and workflow design\nsuggestions for future work in HRI. While recent proposals\nhave pushed for higher standards of reporting methods, results,\nexclusion criteria and recruitment procedures in HRI [2], we\nfocus on improving study design and reporting to include\nthings that are currently not reported in the first place.\nA. Transparency about system capabilities\nHuman-human interaction (HHI) quality is evaluated in\nways that depend on the task being performed, and can be\nframed as a co-operative activity in which utterances are as\ninformative as needed. This works because for many interac-\ntions we can immediately make assumptions about the goal.\nDeficiencies like the ones in our vignettes highlight the\ndifferences between HHI and HRI. We can blame poor HHI\non a lack of common ground on some level \u2013 a lack of\nshared knowledge, differences in personality or what has been\nobserved [15]. These assumptions from HHI do not hold for\nrobots, for which a human cannot assume that a common\nground even exists[47]. Where humans engaging in HHI have\nmechanisms to address and repair interaction trouble on thefly [36], issues in HRI may instead stem from faulty speech\nrecognition [37, 55] or dialogue models [33].\nBecause HRI involves humans, interactions will always\nbe unpredictable. This can stem from preconceived notions\nabout how the robot should respond, its capabilities, and the\nuser\u2019s role in relation to the robot [16, 70]. Our vignettes\nshow that these expectations are often not fully accounted\nfor, and there is no easy method to identify and handle\nthem. The presentation of the robots, including appearance\nand the environment, heavily influences expectations about the\ninteraction and the robot\u2019s capabilities. Therefore, we can use\nthese as grounding mechanisms. Matching user expectations\nto the capabilities of the robot is a precondition of a good\ninteraction, but may still lead to a poor interaction if the\ncapabilities are very limited.\nUser expectations can be tapped via pre-experiment ques-\ntionnaires, similar to demographic information. In a laboratory\nsetting, we propose that research focus more on qualitative\nmeasures such as subject interviews, co-design and even\ninformal discussions as a form of analysis on the same level as\nquantitative surveys. Through these we can better understand\nuser expectations which are likely to change over time.\nB. Reporting unexpected behaviours\nSome HRI evaluation standards have been used across the\nliterature as highlighted in Section II-B. These are required in\nthe field since they are validated measures that can be used for\ncomparisons over different contexts. On the other hand, many\nquantitative metrics will not capture the interactions described\nin Section IV partially because they may not necessarily\nhappen frequently. Such cases can be used to describe flaws\nin robot or interaction design that still need to be tackled in\nthe further development of dialogue systems for HRI. We\ntherefore call for a critical perspective that does not only\npresent the benefits and successes in using novel technology\nsuch as LLMs, but also seriously discusses its challenges.\nWe found the approach of writing vignettes useful for\ntalking about unexpected behaviours of systems that we ex-\nperienced. Formulating and narrating a scenario enabled us to\ndiscuss more concretely why the interaction was perceived\nthat way, and what the underlying cause may have been.\nWe propose that the HRI and dialogue system communities\ncould benefit from using this approach to present system\nmalfunctions at greater scale than what is currently done.\nVII. C ONCLUSIONS\nIn the future, we would like interactions between humans\nand robots to be natural, effective, honest and clear about\ntheir purpose, goal and capabilities. These are not easily\nmeasured or evaluated facets of an interaction, but by reporting\nHRI studies as a study of the robots\u2019 behaviours, including\nits failures, rather than as promotional materials for a fully\nfunctional system, we can move towards those goals.\nACKNOWLEDGMENTS\nThe authors would like to thank Felix Gervits for providing\nimportant feedback on an earlier version of this article. We\n\n--- Page 7 ---\nwould also like to thank the designers of the systems men-\ntioned in Vignette 5 and Vignette 6 for giving us permission\nto use their systems as examples in this paper. This research\nwas (partially) funded by the Hybrid Intelligence Center, a 10-\nyear programme funded by the Dutch Ministry of Education,\nCulture and Science through the Netherlands Organisation for\nScientific Research, grant number 024.004.022. This work was\nsupported by JST Moonshot R&D JPMJPS2011.\nREFERENCES\n[1] Jens Allwood, Joakim Nivre, and Elisabeth Ahls \u00b4en. On\nthe Semantics and Pragmatics of Linguistic Feedback.\nJournal of Semantics , 9(1):1\u201326, 01 1992. ISSN 0167-\n5133. doi: 10.1093/jos/9.1.1. URL https://doi.org/10.1\n093/jos/9.1.1.\n[2] Shelly Bagchi, Patrick Holthaus, Gloria Beraldo, Em-\nmanuel Senft, Daniel Hernandez Garcia, Zhao Han,\nSuresh Kumaar Jayaraman, Alessandra Rossi, Con-\nnor Esterwood, Antonio Andriella, and Paul Prid-\nham. Towards Improved Replicability of Human Stud-\nies in Human-Robot Interaction: Recommendations for\nFormalized Reporting. In Companion of the 2023\nACM/IEEE International Conference on Human-Robot\nInteraction , HRI \u201923, page 629\u2013633, New York, NY ,\nUSA, 2023. Association for Computing Machinery.\nISBN 9781450399708. doi: 10.1145/3568294.3580162.\nURL https://doi.org/10.1145/3568294.3580162.\n[3] Christoph Bartneck, Dana Kuli \u00b4c, Elizabeth Croft, and\nSusana Zoghbi. Measurement instruments for the anthro-\npomorphism, animacy, likeability, perceived intelligence,\nand perceived safety of robots. International journal of\nsocial robotics , 1(1):71\u201381, 2009.\n[4] Stephanie Behm Cross. Whiteness in the academy: using\nvignettes to move beyond safe silences. Teaching in\nHigher Education , 22(7):879\u2013887, October 2017. ISSN\n1356-2517, 1470-1294. doi: 10.1080/13562517.2017.13\n40266. URL https://www.tandfonline.com/doi/full/10.10\n80/13562517.2017.1340266.\n[5] Kathleen Belhassein, V \u00b4\u0131ctor Fern \u00b4andez-Castro, Aman-\ndine Mayima, Aur \u00b4elie Clodic, Elisabeth Pacherie,\nMich `ele Guidetti, Rachid Alami, and H \u00b4el`ene Cochet.\nAddressing joint action challenges in HRI: Insights from\npsychology and philosophy. Acta Psychologica , 222:\n103476, 2022. ISSN 0001-6918. doi: https://doi.org/\n10.1016/j.actpsy.2021.103476. URL https://www.scienc\nedirect.com/science/article/pii/S0001691821002262.\n[6] Emily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. On the Dangers\nof Stochastic Parrots: Can Language Models Be Too\nBig? In Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency , FAccT \u201921,\npage 610\u2013623, New York, NY , USA, 2021. Association\nfor Computing Machinery. ISBN 9781450383097. doi:\n10.1145/3442188.3445922. URL https://doi.org/10.114\n5/3442188.3445922.[7] James M. Berzuk and James E. Young. More Than\nWords: A Framework for Describing Human-Robot Di-\nalog Designs. In Proceedings of the 2022 ACM/IEEE\nInternational Conference on Human-Robot Interaction ,\nHRI \u201922, page 393\u2013401, New York, USA, 2022. IEEE\nPress.\n[8] Anna Bloom-Christen and Hendrikje Grunow. What\u2019s\n(in) a Vignette? History, Functions, and Development of\nan Elusive Ethnographic Sub-genre. Ethnos , 0(0):1\u201319,\nMarch 2022. ISSN 0014-1844, 1469-588X. doi: 10.108\n0/00141844.2022.2052927. URL https://www.tandfonlin\ne.com/doi/full/10.1080/00141844.2022.2052927.\n[9] Rishi Bommasani, Kevin Klyman, Shayne Longpre,\nSayash Kapoor, Nestor Maslej, Betty Xiong, Daniel\nZhang, and Percy Liang. The Foundation Model Trans-\nparency Index, 2023.\n[10] Simone Borsci, Alessio Malizia, Martin Schmettow,\nFrank Van Der Velde, Gunay Tariverdiyeva, Divyaa\nBalaji, and Alan Chamberlain. The Chatbot Usability\nScale: the design and pilot of a usability scale for\ninteraction with AI-based conversational agents. Personal\nand Ubiquitous Computing , 26:95\u2013119, 2022.\n[11] C. Breazeal. Social interactions in HRI: the robot view.\nIEEE Transactions on Systems, Man, and Cybernetics,\nPart C (Applications and Reviews) , 34(2):181\u2013186, 2004.\ndoi: 10.1109/TSMCC.2004.826268.\n[12] Daniel J Brooks. A human-centric approach to au-\ntonomous robot failures . PhD thesis, University of\nMassachusetts Lowell, 2017.\n[13] Joana Campos, James Kennedy, and Jill F. Lehman.\nChallenges in Exploiting Conversational Memory in\nHuman-Agent Interaction. In Proceedings of the 17th\nInternational Conference on Autonomous Agents and\nMultiAgent Systems , AAMAS \u201918, page 1649\u20131657,\nRichland, SC, 2018. International Foundation for Au-\ntonomous Agents and Multiagent Systems.\n[14] Xiaoyu Chang, Yanheng Li, Sijia Liu, Ling Ma, and\nRay Lc. \u201dSorry to Keep You Waiting\u201d: Recovering\nfrom Negative Consequences Resulting from Service\nRobot Unintended Rejection. In Proceedings of the\n2024 ACM/IEEE International Conference on Human-\nRobot Interaction , HRI \u201924, page 96\u2013105, New York,\nNY , USA, 2024. Association for Computing Machinery.\nISBN 9798400703225. doi: 10.1145/3610977.3634959.\nURL https://doi.org/10.1145/3610977.3634959.\n[15] Herbert H Clark. Using language . Cambridge University\nPress, Cambridge, UK, 1996. ISBN 9780521567459.\n[16] Herbert H. Clark and Kerstin Fischer. Social robots as\ndepictions of social agents. Behavioral and Brain Sci-\nences , 46:e21, 2023. doi: 10.1017/S0140525X22000668.\n[17] Enrique Coronado, Takuya Kiyokawa, Gustavo A Garcia\nRicardez, Ixchel G Ramirez-Alpizar, Gentiane Venture,\nand Natsuki Yamanobe. Evaluating quality in human-\nrobot interaction: A systematic search and classification\nof performance and human-centered factors, measures\nand metrics towards an industry 5.0. Journal of Man-\n\n--- Page 8 ---\nufacturing Systems , 63:392\u2013410, 2022.\n[18] Filipa Correia, Carla Guerra, Samuel Mascarenhas, Fran-\ncisco S. Melo, and Ana Paiva. Exploring the Impact of\nFault Justification in Human-Robot Trust. In Proceedings\nof the 17th International Conference on Autonomous\nAgents and MultiAgent Systems , AAMAS \u201918, page\n507\u2013513, Richland, SC, 2018. International Foundation\nfor Autonomous Agents and Multiagent Systems.\n[19] Andy Crabtree, Mark Rouncefield, and Peter Tolmie.\nDoing design ethnography . Springer, London, 2012.\nISBN 978-1-4471-2726-0.\n[20] Viviene E. Cree. \u2018I\u2019d Like to Call You My Mother.\u2019\nReflections on Supervising International PhD Students\nin Social Work. Social Work Education , 31(4):451\u2013464,\nJune 2012. ISSN 0261-5479, 1470-1227. doi: 10.1080/\n02615479.2011.562287. URL http://www.tandfonline.co\nm/doi/abs/10.1080/02615479.2011.562287.\n[21] Mirjam De Haas, Veerle Hobbelink, and Matthijs Smak-\nman. Inclusive Dialogues: WokeBot Engaging Diversity\nDilemmas. In Companion of the 2024 ACM/IEEE Inter-\nnational Conference on Human-Robot Interaction , HRI\n\u201924, page 379\u2013382, New York, NY , USA, 2024. Associ-\nation for Computing Machinery. ISBN 9798400703232.\ndoi: 10.1145/3610978.3640650. URL https://doi.org/10\n.1145/3610978.3640650.\n[22] Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo\nEchegoyen, Sophie Rosset, Eneko Agirre, and Mark\nCieliebak. Survey on evaluation methods for dialogue\nsystems. Artificial Intelligence Review , 54:755\u2013810,\n2021.\n[23] Munjal Desai, Mikhail Medvedev, Marynel V \u00b4azquez,\nSean McSheehy, Sofia Gadea-Omelchenko, Christian\nBruggeman, Aaron Steinfeld, and Holly Yanco. Effects\nof changing reliability on trust of robot systems. In\nProceedings of the Seventh Annual ACM/IEEE Interna-\ntional Conference on Human-Robot Interaction , HRI \u201912,\npage 73\u201380, New York, NY , USA, 2012. Association\nfor Computing Machinery. ISBN 9781450310635. doi:\n10.1145/2157689.2157702. URL https://doi.org/10.114\n5/2157689.2157702.\n[24] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha\nRavichander, Eduard Hovy, Hinrich Sch \u00a8utze, and Yoav\nGoldberg. Measuring and improving consistency in pre-\ntrained language models. Transactions of the Association\nfor Computational Linguistics , 9:1012\u20131031, 2021.\n[25] Robert M Emerson, Rachel I Fretz, and Linda L Shaw.\nWriting ethnographic fieldnotes . University of Chicago\npress, Chicago 60637, 2011.\n[26] Siska Fitrianie, Merijn Bruijnes, Deborah Richards, Amal\nAbdulrahman, and Willem-Paul Brinkman. What Are\nWe Measuring Anyway? - A Literature Survey of Ques-\ntionnaires Used in Studies Reported in the Intelligent\nVirtual Agent Conferences. In Proceedings of the 19th\nACM International Conference on Intelligent Virtual\nAgents , IV A \u201919, page 159\u2013161, New York, NY , USA,\n2019. Association for Computing Machinery. ISBN9781450366724. doi: 10.1145/3308532.3329421. URL\nhttps://doi.org/10.1145/3308532.3329421.\n[27] Siska Fitrianie, Merijn Bruijnes, Fengxiang Li, Amal Ab-\ndulrahman, and Willem-Paul Brinkman. The Artificial-\nSocial-Agent Questionnaire: Establishing the Long and\nShort Questionnaire Versions. In Proceedings of the 22nd\nACM International Conference on Intelligent Virtual\nAgents , IV A \u201922, New York, NY , USA, 2022. Association\nfor Computing Machinery. ISBN 9781450392488. doi:\n10.1145/3514197.3549612. URL https://doi.org/10.114\n5/3514197.3549612.\n[28] Jodi Forlizzi, Carl DiSalvo, and Francine Gemperle.\nAssistive robotics and an ecology of elders living inde-\npendently in their homes. Human\u2013Computer Interaction ,\n19(1-2):25\u201359, 2004.\n[29] Mary Ellen Foster, Andre Gaschler, Manuel Giuliani,\nAmy Isard, Maria Pateraki, and Ronald P.A. Petrick. Two\nPeople Walk into a Bar: Dynamic Multi-Party Social\nInteraction with a Robot Agent. In Proceedings of\nthe 14th ACM International Conference on Multimodal\nInteraction , ICMI \u201912, page 3\u201310, New York, NY , USA,\n2012. Association for Computing Machinery. ISBN\n9781450314671. doi: 10.1145/2388676.2388680. URL\nhttps://doi.org/10.1145/2388676.2388680.\n[30] Matthew L. Ginsberg and David E. Smith. Reasoning\nabout action II: The qualification problem. Artificial\nIntelligence , 35(3):311\u2013342, 1988. ISSN 0004-3702. doi:\nhttps://doi.org/10.1016/0004-3702(88)90020-3. URL\nhttps://www.sciencedirect.com/science/article/pii/000437\n0288900203.\n[31] Erving Goffman. On Fieldwork. Journal of Contem-\nporary Ethnography , 18(2):123\u2013132, July 1989. ISSN\n0891-2416, 1552-5414. doi: 10.1177/08912418901800\n2001. URL http://journals.sagepub.com/doi/10.1177/089\n124189018002001.\n[32] H P Grice. Logic and Conversation. Syntax and\nSemantics , 3:41 \u2013 58, 1975.\n[33] Shanee Honig and Tal Oron-Gilad. Understanding and\nresolving failures in human-robot interaction: Literature\nreview and model development. Frontiers in Psychology ,\n9, 2018. ISSN 1664-1078. doi: 10.3389/fpsyg.2018.008\n61. URL https://www.frontiersin.org/articles/10.3389/f\npsyg.2018.00861.\n[34] An Jacobs, Shirley A. Elprama, and Charlotte I. C. Jew-\nell. Evaluating Human-Robot Interaction with Ethnog-\nraphy. In C \u00b4eline Jost, Brigitte Le P \u00b4ev\u00b4edic, Tony Bel-\npaeme, Cindy Bethel, Dimitrios Chrysostomou, Nigel\nCrook, Marine Grandgeorge, and Nicole Mirnig, edi-\ntors, Human-Robot Interaction: Evaluation Methods and\nTheir Standardization , pages 269\u2013286. Springer Interna-\ntional Publishing, Cham, Switzerland, 2020. ISBN 978-\n3-030-42307-0. doi: 10.1007/978-3-030-42307-0 \\11.\nURL https://doi.org/10.1007/978-3-030-42307-0 11.\n[35] Alice Juel Jacobsen. Vignettes of interviews to enhance\nan ethnographic account. Ethnography and Education , 9\n(1):35\u201350, 2014. doi: 10.1080/17457823.2013.828475.\n\n--- Page 9 ---\nURL https://doi.org/10.1080/17457823.2013.828475.\n[36] Gail Jefferson. Repairing the Broken Surface of Talk:\nManaging Problems in Speaking, Hearing, and Under-\nstanding in Conversation . Oxford University Press, 198\nMadison Avenue, New York, NY 10016, USA, 2018.\nISBN 978-0-19-069796-9.\n[37] James Kennedy, S \u00b4everin Lemaignan, Caroline Mon-\ntassier, Pauline Lavalade, Bahar Irfan, Fotios Papadopou-\nlos, Emmanuel Senft, and Tony Belpaeme. Child Speech\nRecognition in Human-Robot Interaction: Evaluations\nand Recommendations. In Proceedings of the 2017\nACM/IEEE International Conference on Human-Robot\nInteraction , HRI \u201917, page 82\u201390, New York, NY , USA,\n2017. Association for Computing Machinery. ISBN\n9781450343367. doi: 10.1145/2909824.3020229. URL\nhttps://doi.org/10.1145/2909824.3020229.\n[38] Casey Kennington, Malihe Alikhani, Heather Pon-Barry,\nKatherine Atwell, Yonatan Bisk, Daniel Fried, Felix\nGervits, Zhao Han, Mert Inan, Michael Johnston, Raj\nKorpan, Diane Litman, Matthew Marge, Cynthia Ma-\ntuszek, Ross Mead, Shiwali Mohan, Raymond Mooney,\nNatalie Parde, Jivko Sinapov, Angela Stewart, Matthew\nStone, Stefanie Tellex, and Tom Williams. Dialogue\nwith Robots: Proposals for Broadening Participation and\nResearch in the SLIV AR Community, 2024. URL https:\n//arxiv.org/abs/2404.01158.\n[39] Callie Y . Kim, Christine P. Lee, and Bilge Mutlu.\nUnderstanding Large-Language Model (LLM)-powered\nHuman-Robot Interaction. In Proceedings of the 2024\nACM/IEEE International Conference on Human-Robot\nInteraction , HRI \u201924, page 371\u2013380, New York, NY ,\nUSA, 2024. Association for Computing Machinery.\nISBN 9798400703225. doi: 10.1145/3610977.3634966.\nURL https://doi.org/10.1145/3610977.3634966.\n[40] A. Baki Kocaballi, Liliana Laranjo, and Enrico Coiera.\nMeasuring User Experience in Conversational Interfaces:\nA Comparison of Six Questionnaires. In Proceedings of\nthe 32nd International BCS Human Computer Interac-\ntion Conference , HCI \u201918, Swindon, GBR, 2018. BCS\nLearning & Development Ltd. doi: 10.14236/ewic/HCI2\n018.21. URL https://doi.org/10.14236/ewic/HCI2018.21.\n[41] Laura Kunold, Nikolai Bock, and Astrid Rosenthal-\nvon der P \u00a8utten. Not All Robots Are Evaluated Equally:\nThe Impact of Morphological Features on Robots\u2019 As-\nsessment through Capability Attributions. ACM Trans-\nactions on Human-Robot Interaction , 12(1):1\u201331, 2023.\n[42] Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, and\nGary Geunbae Lee. Hybrid approach to robust dialog\nmanagement using agenda and dialog examples. Com-\nputer Speech & Language , 24(4):609\u2013631, 2010.\n[43] Pierre Lison. A hybrid approach to dialogue manage-\nment based on probabilistic rules. Computer Speech &\nLanguage , 34(1):232\u2013255, 2015.\n[44] Paul Luff, Nigel Gilbert, and David Frohlich, editors.\nComputers and Conversation . Computers and People\nSeries. Academic Press, London, 1990. doi: https://doi.org/10.1016/C2009-0-21641-2. URL https:\n//doi.org/10.1016/C2009-0-21641-2.\n[45] Matthew Marge, Carol Espy-Wilson, Nigel G. Ward,\nAbeer Alwan, Yoav Artzi, Mohit Bansal, Gil Blanken-\nship, Joyce Chai, Hal Daum \u00b4e, Debadeepta Dey, Mary\nHarper, Thomas Howard, Casey Kennington, Ivana\nKruijff-Korbayov \u00b4a, Dinesh Manocha, Cynthia Matuszek,\nRoss Mead, Raymond Mooney, Roger K. Moore, Mari\nOstendorf, Heather Pon-Barry, Alexander I. Rudnicky,\nMatthias Scheutz, Robert St. Amant, Tong Sun, Stefanie\nTellex, David Traum, and Zhou Yu. Spoken language\ninteraction with robots: Recommendations for future re-\nsearch. Computer Speech & Language , 71:101255, 2022.\nISSN 0885-2308. doi: https://doi.org/10.1016/j.csl.2021\n.101255. URL https://www.sciencedirect.com/science/ar\nticle/pii/S0885230821000620.\n[46] Amandine Mayima, Aur \u00b4elie Clodic, and Rachid Alami.\nTowards Robots able to Measure in Real-time the Quality\nof Interaction in HRI Contexts. International Journal of\nSocial Robotics , 14(3):713\u2013731, 2022.\n[47] Erik Miehling, Manish Nagireddy, Prasanna Sattigeri,\nElizabeth M. Daly, David Piorkowski, and John T.\nRichards. Language Models in Dialogue: Conversational\nMaxims for Human-AI Interactions, 2024. URL https:\n//arxiv.org/abs/2403.15115.\n[48] Pierrick Milhorat, Divesh Lala, Koji Inoue, Tianyu Zhao,\nMasanari Ishida, Katsuya Takanashi, Shizuka Nakamura,\nand Tatsuya Kawahara. A conversational dialogue man-\nager for the humanoid robot ERICA. In Advanced Social\nInteraction with Agents , pages 119\u2013131. Springer, Cham,\nSwitzerland, 2019.\n[49] Bilge Mutlu and Jodi Forlizzi. Robots in organiza-\ntions: the role of workflow, social, and environmental\nfactors in human-robot interaction. In Proceedings of\nthe 3rd ACM/IEEE International Conference on Human\nRobot Interaction , HRI \u201908, page 287\u2013294, New York,\nNY , USA, 2008. Association for Computing Machinery.\nISBN 9781605580173. doi: 10.1145/1349822.1349860.\nURL https://doi.org/10.1145/1349822.1349860.\n[50] Hannah R.M. Pelikan and Mathias Broth. Why That\nNao? How Humans Adapt to a Conventional Humanoid\nRobot in Taking Turns-at-Talk. In Proceedings of the\n2016 CHI Conference on Human Factors in Comput-\ning Systems , CHI \u201916, page 4921\u20134932, New York,\nNY , USA, 2016. Association for Computing Machinery.\nISBN 9781450333627. doi: 10.1145/2858036.2858478.\nURL https://doi.org/10.1145/2858036.2858478.\n[51] Kathrin Pollmann, Wulf Loh, Nora Fronemann, and\nDaniel Ziegler. Entertainment vs. manipulation: Person-\nalized human-robot interaction between user experience\nand ethical design. Technological Forecasting and Social\nChange , 189:122376, 2023. ISSN 0040-1625. doi:\nhttps://doi.org/10.1016/j.techfore.2023.122376. URL\nhttps://www.sciencedirect.com/science/article/pii/S00401\n62523000616.\n[52] Martin Porcheron, Joel E Fischer, Stuart Reeves, and\n\n--- Page 10 ---\nSarah Sharples. V oice Interfaces in Everyday Life. In\nProceedings of the 2018 CHI Conference on Human\nFactors in Computing Systems , CHI \u201918, pages 640:1\u2014\u2013\n640:12, New York, NY , USA, 2018. ACM. ISBN 978-\n1-4503-5620-6. doi: 10.1145/3173574.3174214. URL\nhttp://doi.acm.org/10.1145/3173574.3174214.\n[53] Merle M. Reimann, Florian A. Kunneman, Catharine\nOertel, and Koen V . Hindriks. A Survey on Dialogue\nManagement in Human-robot Interaction. J. Hum.-Robot\nInteract. , 13(2), jun 2024. doi: 10.1145/3648605. URL\nhttps://doi.org/10.1145/3648605.\n[54] Alessandra Rossi, Kerstin Dautenhahn, Kheng Lee Koay,\nand Michael L Walters. How the timing and magni-\ntude of robot errors influence peoples\u2019 trust of robots\nin an emergency scenario. In Social Robotics: 9th\nInternational Conference, ICSR 2017, Tsukuba, Japan,\nNovember 22-24, 2017, Proceedings 9 , pages 42\u201352,\nCham, Switzerland, 2017. Springer.\n[55] Damien Rudaz and Christian Licoppe. Public Speech\nRecognition Transcripts as a Configuring Parameter in\nHuman-Agents Interactions. In Proceedings of the 2023\nMP-COSIN Workshop , September 2023.\n[56] Damien Rudaz, Karen Tatarian, Rebecca Stower, and\nChristian Licoppe. From inanimate object to agent:\nImpact of pre-beginnings on the emergence of greetings\nwith a robot. ACM Transactions on Human-Robot\nInteraction , 12(3):1\u201331, 2023.\n[57] Selma Sabanovic, Marek P Michalowski, and Reid Sim-\nmons. Robots in the wild: Observing human-robot social\ninteraction outside the lab. In 9th IEEE International\nWorkshop on Advanced Motion Control, 2006. , pages\n596\u2013601, Cham, Switzerland, 2006. IEEE, IEEE.\n[58] Alessandra Maria Sabelli, Takayuki Kanda, and Norihiro\nHagita. A conversational robot in an elderly care center:\nan ethnographic study. In Proceedings of the 6th Inter-\nnational Conference on Human-Robot Interaction , HRI\n\u201911, page 37\u201344, New York, NY , USA, 2011. Association\nfor Computing Machinery. ISBN 9781450305617. doi:\n10.1145/1957656.1957669. URL https://doi.org/10.114\n5/1957656.1957669.\n[59] Emanuel A Schegloff. Sequencing in Conversational\nOpenings. American Anthropologist , 70(6):1075\u20131095,\nAugust 1968. ISSN 00027294, 15481433. URL http:\n//www.jstor.org/stable/669510.\n[60] Emanuel A. Schegloff. Reflections on Quantification in\nthe Study of Conversation. Research on Language and\nSocial Interaction , 26(1):99\u2013128, 1993. doi: 10.1207/s1\n5327973rlsi2601 5.\n[61] Soyeon Shin, Dahyun Kang, and Sonya Kwak. User-\ncentered Exploration of Robot Design for Hospitals in\nCOVID-19 Pandemic. In Proceedings of the 2022\nACM/IEEE International Conference on Human-Robot\nInteraction , HRI \u201922, page 1040\u20131044, New York, USA,\n2022. IEEE Press.\n[62] Jack Sidnell and Tanya Stivers. The handbook of conver-\nsation analysis , volume 121. John Wiley & Sons, TheAtrium, Southern Gate, Chichester, West Sussex, PO19\n8SQ, UK, 2012.\n[63] Gabriel Skantze. Turn-taking in Conversational Systems\nand Human-Robot Interaction: A Review. Computer\nSpeech & Language , 67:101178, 2021. ISSN 0885-2308.\ndoi: https://doi.org/10.1016/j.csl.2020.101178. URL\nhttps://www.sciencedirect.com/science/article/pii/S08852\n3082030111X.\n[64] Kavyaa Somasundaram, Andrey Kiselev, and Amy\nLoutfi. Intelligent Disobedience: A Novel Approach\nfor Preventing Human Induced Interaction Failures in\nRobot Teleoperation. In Companion of the 2023\nACM/IEEE International Conference on Human-Robot\nInteraction , HRI \u201923, page 142\u2013145, New York, NY ,\nUSA, 2023. Association for Computing Machinery.\nISBN 9781450399708. doi: 10.1145/3568294.3580060.\nURL https://doi.org/10.1145/3568294.3580060.\n[65] Aaron Steinfeld, Terrence Fong, David Kaber, Michael\nLewis, Jean Scholtz, Alan Schultz, and Michael\nGoodrich. Common Metrics for Human-Robot Interac-\ntion. In Proceedings of the 1st ACM SIGCHI/SIGART\nConference on Human-Robot Interaction , HRI \u201906, page\n33\u201340, New York, NY , USA, 2006. Association for\nComputing Machinery. ISBN 1595932941. doi: 10.1\n145/1121241.1121249. URL https://doi.org/10.1145/11\n21241.1121249.\n[66] Wyke Stommel, Lynn de Rijk, and Roel Boumans.\n\u2018Pepper, what do you mean?\u2019 Miscommunication and\nrepair in robot-led survey interaction. In 2022 31st\nIEEE International Conference on Robot and Human\nInteractive Communication (RO-MAN) , page 385\u2013392,\n2022. doi: 10.1109/RO-MAN53752.2022.9900528.\n[67] Lucy A. Suchman. Human-machine reconfigurations:\nPlans and situated actions . Cambridge university press,\nNew York, USA, 2007.\n[68] Jesse Thomason, Shiqi Zhang, Raymond Mooney, and\nPeter Stone. Learning to Interpret Natural Language\nCommands through Human-Robot Dialog. In Proceed-\nings of the 24th International Conference on Artificial\nIntelligence , IJCAI\u201915, page 1923\u20131929, Washington,\nDC, USA, 2015. AAAI Press. ISBN 9781577357384.\n[69] Lucien Tisserand, Brooke Stephenson, Heike Baldauf-\nQuilliatre, Mathieu Lefort, and Fr \u00b4ed\u00b4eric Armetta. Un-\nraveling the thread: Understanding and addressing se-\nquential failures in human-robot interaction anonymous.\nFrontiers in Robotics and AI , 11, 2024. ISSN 2296-\n9144. doi: 10.3389/frobt.2024.1359782. URL\nhttps://www.frontiersin.org/journals/robotics-and-ai/\narticles/10.3389/frobt.2024.1359782.\n[70] Sherry Turkle. Authenticity in the age of digital com-\npanions. Interaction studies , 8(3):501\u2013517, 2007.\n[71] Joseph Weizenbaum. ELIZA\u2014a computer program for\nthe study of natural language communication between\nman and machine. Communications of the ACM , 9(1):\n36\u201345, 1966.\n\n--- Page 11 ---\nAPPENDIX\nA. Vignette-writing instructions\nWe found the following steps helpful to guide us through\nthe writing process.\na) Setting the scene.: Try to recall the context in which\nthe observed instance occurred. Where did it happen? Who\nwas present? In what role were you participating in the event?\nDescribe the scene in such a way that one can imagine being\nthere with you.\nb) Describing details of the action sequence.: This will\nbe the core of your vignette and you now want to zoom in on a\nspecific situation, selecting a single moment in time. Describe\nthe specific person(s) who are interacting with the robot and try\nto recall the order of the events as detailed as you can. You can\nuse direct quotation (she said \u201chow are you?\u201d) when you are\nvery sure that something was phrased in a particular way. You\nmay also use reported speech (she asked me how I was) and\nparaphrasing (we exchanged some greetings). While the focus\nmay be on dialogue here, you also want to describe in detail\nwhat people did, certain movements or facial expressions. If\nyou describe what the robot does, try not to say \u201cthe robot\nexpressed it was happy\u201d but rather be more specific - the robot\nsmiled, played a sound with rising pitch, etc. You can then use\na more interpretative stance in the next step.\nc) Reflecting on what made this reportable: - usually\nthis involves talking about feelings. What makes this event\nstick out as interesting to discuss? What made it surprising,\nfrustrating or otherwise memorable? For whom? This is where\nyou come in with how you experienced what happened from\nyour first person perspective. Try to pinpoint the social norms\nthat the robot was breaching in the situation, for the people\npresent. Often it can help to explicitly formulate what would be\nexpected in this situation and to contrast it with what actually\nhappened. What did you and/or the people present expect to\nhappen? What happened instead and how did you and/or the\npeople present react next?\nEspecially when revising your vignette, try to capture the\norder of events as they evolved (often you just need to move\nsome sentences around). What happened first, and what next?\nKeep the programmers\u2019 perspective out of the vignette\u2014 try to\ndescribe in social terms what happened. If you want, you can\npossibly add a fourth point at the very end if you really want\nto explain what happened from the robot\u2019s perspective. In that\nsection, you should then purely focus on the robot/designer\nperspective, keeping the perspective of the users and the\ndevelopers separate [67].",
  "project_dir": "artifacts/projects/enhanced_cs.HC_2508.10603v1_Why_Report_Failed_Interactions_With_Robots_Towar",
  "communication_dir": "artifacts/projects/enhanced_cs.HC_2508.10603v1_Why_Report_Failed_Interactions_With_Robots_Towar/.agent_comm",
  "assigned_at": "2025-08-17T20:56:11.931135",
  "status": "assigned"
}